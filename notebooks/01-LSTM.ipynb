{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic notebook to implement LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foreword\n",
    "\n",
    "This notebook implements an LSTM model designed for datasets that adhere to a specific structure. The datasets should consist of **time series** data, where each timestamp is associated with sequences of feature values. These sequences represent the behavior of the time series over a defined period.\n",
    "\n",
    "Each sequence must have the same length and should correspond to the timestamp *t* of its **most recent value**. These feature sequences serve as **predictors**, helping the model forecast an **output sequence**â€”which contains the values of the time series counting up to a set time period **ahead** *h* of *t*.\n",
    "\n",
    "As long as these criteria are met, the notebook can be safely executed.\n",
    "\n",
    "In the following cells, LSTM will be performed over data indicating the level of $SO_{2}$ withing a waste incinerator. The goal is to predict to level of $SO_2$ 2 minutes ahead to see if there is any pollution peak coming up and being able to adjust the level of soda ($NaOH$) to control the peaks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly_resampler import FigureResampler\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(df, window_size, horizon, n_steps_in, n_steps_out, features, target):\n",
    "    \"\"\"This function is designed to transform df into a sequential data set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : Pandas DataFrame of shape (n_samples, n_predictors)\n",
    "        Original data frame with the target feature already shifted by 'horizon'\n",
    "\n",
    "    window_size : str\n",
    "        How far we look back to create our sequences for each predictor\n",
    "\n",
    "    horizon : str\n",
    "        How far ahead we try to predict\n",
    "\n",
    "    n_steps_in : int\n",
    "        Length of each predictor sequence\n",
    "\n",
    "    n_steps_out : int\n",
    "        Length of each sequence for the target feature\n",
    "\n",
    "    predictors : list\n",
    "        The predictors\n",
    "\n",
    "    target : list\n",
    "        The target(s)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_sequence : Pandas DataFrame\n",
    "        Sequential data frame\n",
    "    \"\"\"\n",
    "    X, y, ind = [], [], []\n",
    "\n",
    "    # We can create both the predictors and target sequences simultaneously using the shifted 'df'. The key is determining\n",
    "    # whether 'window_size' or 'horizon' is larger. Apply rolling to the larger one, then create the smaller sequence as a\n",
    "    # subset, trimming any excess data.\n",
    "    window = max(pd.Timedelta(window_size), pd.Timedelta(horizon))\n",
    "    max_steps = max(n_steps_in, n_steps_out)\n",
    "    for df_rolling in df.rolling(window=window):\n",
    "        if len(df_rolling) == max_steps:  # Ensure we have the correct window size\n",
    "            seq_x = df_rolling[features].values\n",
    "            seq_y = df_rolling[f\"{target[0]} - TARGET\"].values\n",
    "\n",
    "            # Prune the sequences\n",
    "            if n_steps_in <= n_steps_out:\n",
    "                seq_x = seq_x[-n_steps_in:]\n",
    "            else:\n",
    "                seq_y = seq_y[-n_steps_out:]\n",
    "\n",
    "            # Check for NaN values\n",
    "            if not (np.isnan(seq_x).any() or np.isnan(seq_y).any()):\n",
    "                X.append(seq_x)\n",
    "                y.append(seq_y)\n",
    "                ind.append(df_rolling.index[-1])\n",
    "\n",
    "    # Create DataFrames for the features and target sequences\n",
    "    df_X = pd.DataFrame({f\"{feature}\": [seq[:, i] for seq in X] for i, feature in enumerate(features)}, index=ind)\n",
    "    df_y = pd.DataFrame({f\"{tgt} - TARGET\": [seq for seq in y] for tgt in target}, index=ind)  # Assuming 1D target\n",
    "\n",
    "    # Combine feature and target DataFrames\n",
    "    df_sequence = pd.concat([df_X, df_y], axis=1)\n",
    "\n",
    "    return df_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesDataSet is used to prepare the dataset for training with PyTorch, this class takes care of the variable\n",
    "# transformation, random sampling, and missing value filling, etc.\n",
    "\n",
    "\n",
    "# Source : https://www.kaggle.com/code/aneridalwadi/time-series-with-pytorch\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format your data accordingly to the foreword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give an plain example of preprocessing on a time series that follows the evolution of the bitcoin price. Once the preprocessing done we will run our LSTM. We try to predict a global tendancy of the bitcoin's price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "window_size = \"2min\"  # How far we look back\n",
    "horizon = \"2min\"  # How far ahead we try to predict\n",
    "n_steps_in = 12  # How long each predictor sequence must be to contain all the points within the window\n",
    "n_steps_out = 12  # How long each output sequence must be to contain all the points to arrive at the horizon\n",
    "batch_size = 30  # Number of sequences passed to the LSTM at each iteration (not each epoch !) it allows to update the weights and biases in a more robust manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"tags\": [\"remove-input\"]}\n",
    "predictors = [\n",
    "    \"AT18031.DACA.PV\",\n",
    "    \"AIC16022.PIDA.PV\",\n",
    "    \"AIC16022.PIDA.OP\",\n",
    "    \"AIC16032.PIDA.PV\",\n",
    "    \"AIC16032.PIDA.OP\",\n",
    "]  # Define your predictors columns there\n",
    "target = [\"AT18031.DACA.PV\"]  # Define your target column there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"tags\": [\"remove-input\"]}\n",
    "# Import the data\n",
    "df_to_parse = pd.read_csv(\"../data/paprec-2023-08.csv\", index_col=0).set_index(\"Timestamp\")\n",
    "df_to_parse.index = pd.to_datetime(df_to_parse.index)\n",
    "df_to_parse = df_to_parse.loc[:, predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the predictors from the target\n",
    "X, y = df_to_parse.copy(), df_to_parse[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the predictors\n",
    "ss = StandardScaler()\n",
    "X_scaled = pd.DataFrame(ss.fit_transform(X), index=df_to_parse.index, columns=predictors)\n",
    "\n",
    "# Shift the target values\n",
    "y_shifted = y.shift(freq=f\"-{horizon}\")\n",
    "y_shifted.rename(columns={f\"{target[0]}\": f\"{target[0]} - TARGET\"}, inplace=True)\n",
    "\n",
    "# Initialize the data frame with the scaled values and the target shifted by an hour so that we have in the target column the values\n",
    "# of temperature ahead by an hour (variable horizon).\n",
    "df_lstm = pd.concat([X_scaled, y_shifted], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequence = split_sequences(\n",
    "    df_lstm,\n",
    "    window_size,\n",
    "    horizon,\n",
    "    n_steps_in,\n",
    "    n_steps_out,\n",
    "    predictors,\n",
    "    target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between train and test\n",
    "\n",
    "# Select the training data based on the given date\n",
    "df_train = X[X.index < \"2023-08-07\"].loc[:, predictors]\n",
    "\n",
    "# Find indices where the target value exceeds the threshold\n",
    "matching_indices = df_train[df_train[target[0]] > 50].index\n",
    "list_train_periods = []\n",
    "\n",
    "# Extend each matching index to include 1 hour before and 1 hour after\n",
    "for idx in matching_indices:\n",
    "    start = idx - pd.Timedelta(hours=1.5)\n",
    "    end = idx + pd.Timedelta(hours=1.5)\n",
    "\n",
    "    # Add valid indices to the list\n",
    "    df_idx = df_sequence[(df_sequence.index >= start) & (df_sequence.index <= end)]\n",
    "    list_train_periods.append(df_idx)\n",
    "\n",
    "# Remove duplicates and sort the indices\n",
    "df_train = pd.concat(list_train_periods)\n",
    "mask_idx = df_train.index.duplicated()\n",
    "\n",
    "# Now select the rows in df_sequence using the valid indices\n",
    "X_train = df_train.loc[~mask_idx, predictors]\n",
    "y_train = df_train.loc[~mask_idx, f\"{target[0]} - TARGET\"]\n",
    "\n",
    "# Select the test data as usual\n",
    "X_test = df_sequence[df_sequence.index >= \"2023-08-07\"].loc[:, predictors]\n",
    "y_test = df_sequence[df_sequence.index >= \"2023-08-07\"].loc[:, f\"{target[0]} - TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the train and test since Data Frame does not understand arrays\n",
    "X_train_formatted = np.array([np.stack(row) for row in X_train.to_numpy()])\n",
    "y_train_formatted = np.array([np.stack(row) for row in y_train.to_numpy()])\n",
    "\n",
    "X_test_formatted = np.array([np.stack(row) for row in X_test.to_numpy()])\n",
    "y_test_formatted = np.array([np.stack(row) for row in y_test.to_numpy()])\n",
    "\n",
    "# Convert it to tensor\n",
    "# Setting 'required_grad' to true helps to track all the operations performed on the tensor, essential for performing\n",
    "# back propagation.\n",
    "\n",
    "# Source : https://stackoverflow.com/questions/72113541/utility-of-wrapping-tensor-in-variable-with-requires-grad-false-in-legacy-pytorc\n",
    "X_train_tensors = torch.tensor(X_train_formatted, requires_grad=True).float()\n",
    "y_train_tensors = torch.tensor(y_train_formatted, requires_grad=True).float()\n",
    "X_test_tensors = torch.tensor(X_test_formatted, requires_grad=True).float()\n",
    "y_test_tensors = torch.tensor(y_test_formatted, requires_grad=True).float()\n",
    "\n",
    "# Convert it to torch Dataset\n",
    "train_dataset = TimeSeriesDataset(X_train_tensors, y_train_tensors)\n",
    "test_dataset = TimeSeriesDataset(X_test_tensors, y_test_tensors)\n",
    "\n",
    "# Create train and test loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "Here will be implemented the LSTM..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : https://freedium.cfd/https://charlieoneill.medium.com/predicting-the-price-of-bitcoin-with-multivariate-pytorch-lstms-695bc294130\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size  # output size\n",
    "        self.num_layers = num_layers  # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size  # input size (i.e. the length of each sequence)\n",
    "        self.hidden_size = hidden_size  # LSTM units in each lstm layer (also called the number of neurons in each layer)\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)  # LSTM\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected\n",
    "        self.fc_2 = nn.Linear(128, output_size)  # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden state also called the short-memory term which is the output given at the end of each LSTM unit\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # cell state also called the long-memory term which helps compute the output of each LSTM unit\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        _, (hn, _) = self.lstm(x, (h_0, c_0))  # We only keep the hidden state since it is the output given by the LSTM model\n",
    "        hn = hn[-1]  # Get the hidden features from the last layer of the LSTM\n",
    "        out = self.relu(hn)  # Apply ReLU\n",
    "        out = self.fc_1(out)  # First fully-connect layer\n",
    "        out = self.relu(out)  # Apply ReLU\n",
    "        out = self.fc_2(out)  # Second fully-connect layer (output layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, delta):\n",
    "        self.patience = patience  # Max number of epoch allowed with no improvement\n",
    "        self.delta = delta  # Minimum improvement that should observed between two epochs to reset the patience\n",
    "        self.best_score = None\n",
    "        self.early_stop = False  # Wether we perform early stopping or not\n",
    "        self.counter = 0  # Count the number of epoch of no improvement after the last improvement\n",
    "        self.best_model_state = None  # Save the best model parameters\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score > (100 - self.delta) / 100 * self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 1 : https://pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc\n",
    "# Source 2 : https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, loss_function, epoch, optimizer):\n",
    "    \"\"\"This function trains the model on the train loader using batch iteration.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    model : LSTM\n",
    "        LSTM model\n",
    "\n",
    "    train_loader : DataLoader\n",
    "        Contains the train data in a DataLoader format\n",
    "\n",
    "    loss_function : PyTorch loss function\n",
    "        A loss function to assess how the model performed on each batch\n",
    "\n",
    "    epoch : int\n",
    "        Indicates the current epoch\n",
    "\n",
    "    optimizer : PyTorch optimizer\n",
    "        Optimizer used to adjust the weights minimizing the loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss_across_batches : float\n",
    "        Returns the average loss for the last 100 (or less if the epoch has not gone through at least 100) batches\n",
    "    \"\"\"\n",
    "\n",
    "    # Even if at first sight we do not absolutely need it, it is a good practice because I quote 'because the module you are using\n",
    "    # might be updated to behave differently in training and eval modes' (cf. Source 1).\n",
    "    model.train(True)  # Set the model to 'training mode'. It allows dropout layers to set a fraction of each batch to 0\n",
    "    # On 'training mode' batch normalization layer compute the mean and the variance to improve gradient flow and avoid\n",
    "    # exploding/ vanishing gradient descent (cf. Source 1 - 2)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    running_loss = 0.0\n",
    "    one_batch = False  # See if it completes at least 100 batches\n",
    "\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        x_batch, y_batch = batch[0], batch[1]\n",
    "        output = model(x_batch)\n",
    "        loss = loss_function(output, y_batch)\n",
    "        loss_sqrt = torch.sqrt(loss)\n",
    "        running_loss += loss_sqrt.item()\n",
    "\n",
    "        optimizer.zero_grad()  # Resets the gradient before computing back propagation\n",
    "        loss.backward()  # Computes back propagation\n",
    "        optimizer.step()  # Updates the model parameters\n",
    "\n",
    "        if batch_index % 100 == 99:  # Print every 100 batches\n",
    "            one_batch = True\n",
    "            avg_loss_across_batches = running_loss / 100\n",
    "            print(\n",
    "                \"Batch {0}, Sqrt {1} : {2:.3f}\".format(batch_index + 1, type(loss_function).__name__, avg_loss_across_batches)\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "    print()\n",
    "    if not one_batch:\n",
    "        avg_loss_across_batches = running_loss / batch_index\n",
    "    return avg_loss_across_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
    "def validate_one_epoch(model, test_loader, loss_function):\n",
    "    \"\"\"This functions computes and prints the square root of the average loss across all the batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LSTM\n",
    "        LSTM model\n",
    "\n",
    "    test_loader : DataLoader\n",
    "        Contains the test data in a DataLoader format\n",
    "\n",
    "    loss_function : PyTorch loss function\n",
    "        A loss function to assess how the model performed on each batch\n",
    "    \"\"\"\n",
    "    model.train(False)  # Set it to False since we are in evaluation mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for _, batch in enumerate(test_loader):\n",
    "        x_batch, y_batch = batch[0], batch[1]\n",
    "\n",
    "        with torch.no_grad():  # No gradient descent needed since we are in evaluation mode we use the weigths and biases fitted in the train\n",
    "            output = model(x_batch)\n",
    "            loss = loss_function(output, y_batch)\n",
    "            loss_sqrt = torch.sqrt(loss)\n",
    "            running_loss += loss_sqrt.item()\n",
    "\n",
    "    avg_loss_across_batches = running_loss / len(test_loader)\n",
    "\n",
    "    print(\"Val Sqrt. {0} : {1:.3f}\".format(type(loss_function).__name__, avg_loss_across_batches))\n",
    "    print(\"***************************************************\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM parameters\n",
    "input_size = 12\n",
    "output_size = 12\n",
    "hidden_size = 10\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = LSTM(input_size=input_size, output_size=output_size, hidden_size=hidden_size, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "early_stopping = True\n",
    "\n",
    "if early_stopping:\n",
    "    patience = 100\n",
    "    delta = 3\n",
    "    es = EarlyStopping(patience=patience, delta=delta)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    val_loss = train_one_epoch(model, train_loader, loss_function, epoch, optimizer)  # Train\n",
    "    if early_stopping:\n",
    "        es(val_loss, model)\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        else:\n",
    "            validate_one_epoch(model, test_loader, loss_function)  # Test\n",
    "    else:\n",
    "        validate_one_epoch(model, test_loader, loss_function)  # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # We use torch.no_grad() here since we apply the model upon data using the weights and biases computed during the training period\n",
    "    predicted_train = model(X_train_tensors).numpy()\n",
    "    predicted_test = model(X_test_tensors).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 6))\n",
    "plt.scatter(\n",
    "    y_train_tensors.detach()\n",
    "    .numpy()[:, 1]\n",
    "    .ravel(),  # We convert the tensor to a numpy array for the plot since the tensor is in the right format\n",
    "    predicted_train[:, -1],\n",
    "    label=\"Predicted TARGET\",\n",
    ")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_train_tensors.detach().numpy()[:, 1].ravel(), predicted_train[:, -1]))\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(\n",
    "        np.min(y_train_tensors.detach().numpy()[:, 1].ravel()),\n",
    "        np.max(y_train_tensors.detach().numpy()[:, 1].ravel()),\n",
    "    ),\n",
    "    np.arange(\n",
    "        np.min(y_train_tensors.detach().numpy()[:, 1].ravel()),\n",
    "        np.max(y_train_tensors.detach().numpy()[:, 1].ravel()),\n",
    "    ),\n",
    "    linestyle=\"--\",\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"TARGET - True\")\n",
    "plt.ylabel(\"TARGET Predicted\")\n",
    "plt.title(label=f\"LSTM performance on train data \\nRMSE = {rmse:.3f} \\nLSTM parameters : {model.__dict__['_modules']['lstm']}\")\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 6))\n",
    "plt.scatter(\n",
    "    y_test_tensors.detach()\n",
    "    .numpy()[:, 1]\n",
    "    .ravel(),  # We convert the tensor to a numpy array for the plot since the tensor is in the right format\n",
    "    predicted_test[:, -1],\n",
    "    label=\"Predicted TARGET\",\n",
    ")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_tensors.detach().numpy()[:, 1].ravel(), predicted_test[:, -1]))\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(\n",
    "        np.min(y_test_tensors.detach().numpy()[:, 1].ravel()),\n",
    "        np.max(y_test_tensors.detach().numpy()[:, 1].ravel()),\n",
    "    ),\n",
    "    np.arange(\n",
    "        np.min(y_test_tensors.detach().numpy()[:, 1].ravel()),\n",
    "        np.max(y_test_tensors.detach().numpy()[:, 1].ravel()),\n",
    "    ),\n",
    "    linestyle=\"--\",\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"TARGET - True\")\n",
    "plt.ylabel(\"TARGET - Predicted\")\n",
    "plt.title(label=f\"LSTM performance on test data \\nRMSE = {rmse:.3f} \\nLSTM parameters : {model.__dict__['_modules']['lstm']}\")\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = FigureResampler(make_subplots(rows=1, cols=1))\n",
    "\n",
    "y_pred_train_trace = go.Scatter(\n",
    "    x=X_train.index,\n",
    "    y=predicted_train[:, -1].ravel(),\n",
    "    mode=\"lines+markers\",\n",
    "    marker_symbol=\"x\",\n",
    "    marker_color=\"blue\",\n",
    "    name=\"TARGET - Train predictions\",\n",
    ")\n",
    "fig.add_trace(y_pred_train_trace)\n",
    "\n",
    "y_pred_test_trace = go.Scatter(\n",
    "    x=X_test.index,\n",
    "    y=predicted_test[:, -1].reshape(-1, 1).ravel(),\n",
    "    mode=\"lines+markers\",\n",
    "    marker_symbol=\"square\",\n",
    "    marker_color=\"blue\",\n",
    "    name=\"TARGET - Test predictions\",\n",
    ")\n",
    "\n",
    "fig.add_trace(y_pred_test_trace)\n",
    "\n",
    "y_true_trace = go.Scatter(\n",
    "    x=df_to_parse.index,\n",
    "    y=df_to_parse.loc[:, target].values.ravel(),\n",
    "    mode=\"lines+markers\",\n",
    "    marker_color=\"green\",\n",
    "    name=\"TARGET - Raw signal\",\n",
    ")\n",
    "fig.add_trace(y_true_trace)\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text=\"TARGET\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Timeline\", row=1, col=1)\n",
    "fig.update_layout(template=\"plotly_white\", height=600, title=\"TARGET predictions compared to true data\")\n",
    "fig.show_dash(port=8054)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm-NH0BiH-O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
